---
title: "patRoon tutorial"
author: "Rick Helmus"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{patRoon tutorial}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  warning = FALSE,
  error = FALSE,
  message = FALSE
)

options(patRoon.cache.fileName = "~/tutorial.sqlite")

options(width=250) # big nr to avoid wrapping text in code chunks
```

<style>
pre, code {
    white-space: pre !important;
    overflow-x: auto !important;
}
</style>

## Introduction

The intro ...

### Workflow

## Data

In this tutorial we will use example data provided within the [patRoonData] package. Please make sure this package is installed (see the [readme] for brief installation instructions). The example dataset contains LC-MS data for a standard mixture with known composition ('standard-X') and a blank solvent ('solvent-X'), both in triplicate. While this may not seem like the most exciting data, it does allow to demonstrate the most important functionalities of `patRoon`.

The provided analyses already have been exported to an open format (_.mzML_) and are ready to use. For your own data it may be necessary to first  export your data to _mzXML_ or _mzML_ and perform other data pre-treatment steps such as mass recalibration. This can be done using the tools from [ProteoWizard] or software from your mass spectrometer vendor. Alternatively, `patRoon` can do this automatically for your analyses with the `convertMSFiles()` function. Please see the reference manual or other vignettes for its usage.

## New project

Whenever you start a new non-target analysis it is highly recommended to start this from a fresh _project directory_. This directory will contain your `R` processing script(s) and any other output generated during the workflow. Note that this directory does not have to contain the raw MS data files. In fact, keeping these files separate may be handy, for instance, if you want to run multiple non-target analyses on these files or store the analysis files on a shared location.

Starting a new project typically conists of

1. Creating a new directory (unsurprisingly!)
2. Changing the active working directory to he project directory (e.g. with `setwd()`).
3. Create (or copy) an `R` processing script.

Note that step 2 is important as any output files (e.g. reports and cached results) are stored to the current working directory by default. Consequenly, always take care to ensure that this directory is active, for instance, after restarting `R`.

Steps 1-3 can be easily performed with the `newProject()` function. Alternatively, you can of course also perform these steps yourself. Both approaches will be discussed in the next sections.

### Automatic project creation

Ensure that RStudio is active and start the new project utility:

```{r eval=FALSE}
patRoon::newProject()
```

> NOTE: Currently `newProject()` _only_ works when using RStudio.

A dialog should pop-up where you can specify where and how the new project will be generated, which analyses you want to include and define a basic workflow. Based on this input a new project with a template script will be automatically generated.

For this tutorial make the following selections
* **Destination tab** Select your desired location of the new project. Leave other settings as they are.
* **Analyses tab** Here you normally select your analyses. However, for this tutorial simply select the _Example data_ option.
* **Data pre-treatment tab** Since the example data is already ready to use you can simply skip this tab.
* **Features tab** Leave the default OpenMS algorithm for feature finding and grouping.
* **Annotation tab** Select _GenForm_, _MetFrag_ and _mzR_ for the formula generation, compound identification and peak list generator options, respectively (note that the last will become visible when selecting either of the other options). Select _RAMClustR_ for component generation.
* **Reporting tab** You can leave any defaults here.

### Manual project creation

For RStudio users it is easiest to simply create a new RStudio project (e.g. _File_ --> _New Project_). This will create a new directory and ensure that the working directory is set whenever you re-open this project from RStudio. Alternatively, you can do this manually, for intance:
```{r eval=FALSE}
projDir <- "~/myProjectDir"
dir.create(projDir)
setwd(projDir)
```

The next step is to create a new `R` script. For this tutorial simply copy the script that is shown in the next section to a new `.R` file.

### Template R script

After you ran `newProject()` the file below will be created. Before running this script, however, we still have to add and modify some of its code. In the next sections you will learn more about each part of the script, make the necessary changes and run its code.

```{r eval=FALSE}
# insert template script here
```

To start a new non-target analysis it is highly recommended to create a new directory. We will refer to this directory as the _project directory_. This directory typically contains:

* an `R` script (or scripts) which will do the non-target data processing
* an (optional) _.csv_ file with analysis information
* an automatically generated cache file which is internally used by `patRoon` to cache (intermediate) results, therefore, preventing repeated execution of lengthy workflow steps that have been performed before.

Once you have created a suitable directory in the location of your choosing, please create the following skeleton `R` script:

```{r eval=-4}
library(patRoon)

workPath <- "~/my_project" # change this to the location of your project directory
setwd(workPath)

dataDir <- patRoonData::exampleDataPath() # path to example data files
```

The next step is to generate information ncessary for the analysis. To do so, we need to specify the names and file locations of the analyses, to which _replicate group_ they belong and which replicate group(s) should be used for blank subtraction. To do so, we can use the `generateAnalysisInfo()` utility function which automatically generates a suitable `data.frame` for us from given analyses:

```{r}
generateAnalysisInfo(dataDir)
```

As you can see the generated `data.frame` consists of four columns:

* *path*: the path of the file directory containing the analysis
* *analysis*: the name of the analysis. The shoud be the file name _without_ file extension.
* *group*: to which _replicate group_ the analysis belongs. All analysis which are replicates of each other get the same name.
* *ref*: which replicate group should be used for blank subtraction.

The latter two columns are especially important for [data cleanup](#data-cleanup).

In our example, the solvents and standards should belong each to a different replicate group (`"solvent"` and `"standard"`). The solvents should be used for blank subtraction. For this we can use the `groups` and `refs` arguments of `generateAnalysisInfo()`:
```{r}
anaInfo <- generateAnalysisInfo(dataDir,
                                groups = c(rep("solvent", 3), rep("standard", 3)),
                                refs = "solvent")
anaInfo
```

Note that we set the reference for the solvents to themself. This will remove any features from the solvents, which is generally fine as we are usually not interested in the blanks anyway.

Depending on your preference, it may be preferred to export the `data.frame` to a _.csv_ file for larger project so it can be easily edited, for instance, with graphical tools such as Excel. Alternatively, the `newProject()` function can be used to make this process even easier (briefly discussed [here](#newProject)).

## Extract and group features

The first step of a LC-MS non-target analysis workflow is typically the extraction of so called 'features'. While sometimes slightly different definitions are used, a feature can be seen as a single peak within an extracted ion chromatogram. For a complex sample it is not uncommon that hundreds to thousands of features can extracted. Because these large numbers this process is typically automatized nowadays.

To obtain all the features within your dataset the `findFeatures` function is used. This function requires that the analysis information (`anaInfo` variable created earlier) and the desired algorithm that should be used. On top of that there are many more options that can significantly influence the feature finding process, hence, it is important to evaulate results afterwards.

In this tutorial we will use the [OpenMS] software to find features:

```{r features,results='hide'}
fList <- findFeatures(anaInfo, "openms", mzPPM = 5)
```
```{r}
fList
```

After some processing time (especially for larger datasets), the next step is to _group features_. During this step, features from different analysis are grouped by alignment of their retention times and _m/z_ data. This alignment is necessary because it is common that instrumental error will result in (slight) variations for these data between analyses.

To group features the `groupFeatures()` function is used, which has similar argument requirements as `findFeatures` and many more options to tune the process. 

```{r fGroups}
fGroups <- groupFeatures(fList, "openms")
fGroups
```

The `as.data.table()` function can be used to have a look at generated feature groups and their intensities (_i.e._ peak heights) across all analyses:

```{r gTable}
head(as.data.table(fGroups))
```

## Data cleanup { #data-cleanup }

The next step is to perform some basic rule based filtering with the `filter()` function. As its name suggests this function has several way to filter data. It is a so called _generic_ function and _methods_ exists for various data types, such as the feature groups object (`fGroups`) that was made in the previous section.

In this tutorial we will filter grouped features (or 'feature groups') based on a minimal intensity, minimal abundance within replicates and absence within blanks. Finally, we will also remove any feature groups eluting before roughly the dead volume of the system: 

```{r filter}
fGroups <- filter(fGroups, absMinIntensity = 10000, relMinReplicateAbundance = 1,
                  maxReplicateIntRSD = 0.75, blankThreshold = 5, retentionRange = c(110, Inf))
```

As can been from above command this will remove quite a large part our data. However, in our experience using the right settings is a very effective way to separate interesting data from the rest. Some hints on this process:

* The intensity filter is an effective way to remove not only 'noisy' data, but can also be used to remove any low intensity peaks which are very likely to miss any MS/MS data (assuming you are interested in this and data has been recorded with it).
* The `relMinReplicateAbundance` argument specifies a fraction to which a feature should be minimally present within a replicate group. This is a _very_ effective filter in removing any outliers or features which don't actually represent a well defined chromatographic peak, but perhaps are just part of another peak or simply noise.
* The `maxReplicateIntRSD` argument specifies the maximum relative standard deviation (RSD) of feature intensities within a replicate group.
* The `blankThreshold` argument is used for blank subtraction. In the above example, any features with an intensity less than five times compared to the average intensity in blanks will be removed.

Finally, we can have a quick look at our data by plotting some nice extracted ion chromatograms (EICs) for all remaining feature groups:

```{r plotEIC,fig.width=7,results='hide'}
plotEIC(fGroups, colourBy = "fGroups", showFGroupRect = FALSE, showPeakArea = TRUE,
        topMost = 1, showLegend = FALSE)
```

The next logical step in a non-target workflow is most likely to perform further prioritization of data. However, in this tutorial we are just dealing with a relative simple standard mixture, se we will move to the identification part.

## MS peak lists

Having obtained a good dataset with features of interest, we can start moving to figuring out what these may be. Before performing any identification steps, however, the first step is to extract all relevant MS data. This data was obtained with data-dependent MS/MS mode, so in the ideal case we obtain al MS and MS/MS spectra for each feature group.

The `generateMSPeakLists()` will perform this action for us and will generate so called _MS peaklists_ in the process. These peaklists are basicially the relevant MS and MS/MS spectra in tabular form. The command below will use the _mzR_ algorithm to do so:

```{r MSPeakLists,results='hide'}
avgPListParams <- getDefAvgPListParams(clusterMzWindow = 0.005)
plists <- generateMSPeakLists(fGroups, "mzr", maxMSRtWindow = 5, precursorMzWindow = 4,
                              avgFeatParams = avgPListParams, avgFGroupParams = avgPListParams)
```
```{r}
plists
```

## Formula calculation

Using the data from the MS peaklists generated during the previous step we can generate a list of formula candidates for each feature group based on measured _m/z_ values, isotopic patterns and presence of MS/MS fragments. In this tutorial we will use this data as an extra hint to score canddiate chemical structures generated during the next step. The command below will use [GenForm] to perform this step: 

```{r formulas,results='hide'}
formulas <- generateFormulas(fGroups, "genform", plists, relMzDev = 5,
                             adduct = "[M+H]+", elements = "CHNOPSCl",
                             calculateFeatures = TRUE, featThreshold = 0.75)
formulas
```

Note that we limited to only to the the elements C, H, N, O, P, S and Cl. It is highly recommended to limit the elements (by default it is just C, H, N, O and P) as this can significantly reduce processing time and improbable formula candidates. In this tutorial we already knew which compounds to expect so the choice was easy, but often a good guess can be made in advance.

The `generateFormulas()` function returns an object that contains formula candidates assigned for each feature group. In the above call the `calculateFeatures` argument is set to `TRUE`: by doing so formulae are first calculated for individual features within a feature group. These results are then used to generate a consensus candidate formula list for each feature group. During this process any outliers (defined by `featThreshold`) are automatically removed. Setting `calculateFeatures` to `FALSE` instead will calculate formulae directly for feature groups from averaged MS peak lists. This will be significantly faster, but might produce (slightly) less accurate results.

## Compound identification

Now it is time to actually see what compounds we may be dealing with. In this tutorial we will use [MetFrag] to come up with a list of possible candidates structures for each feature group. Before we can start you have to download the [MetFrag CLI jar file][MetFragCL] and specify its file location: 

```{r MF_opt,eval=FALSE}
options(patRoon.path.MetFragCL = "~/MetFrag2.4.3-CL.jar") # change to the full path of the MetFrag CLI jar file
```


Then `generateCompounds()` is used to execute MetFrag and generate the `compounds`.

```{r compounds,results='hide'}
compounds <- generateCompounds(fGroups, plists, "metfrag", topMost = 25,
                               adduct = "[M+H]+", database = "pubchem", maxCandidatesToStop = 5000)
```
```{r fig.width=4,fig.height=4}
compounds
plotSpec(compounds, 1, "M120_R328_91", plists)
```

While `generateCompounds()` is running a list of candidate compound structures will be downloaded for every feature group (in this case from PubChem) and ranked according to various scoring parameters.

As with previous described workfow functions, there are many different arguments that can specified to modify the behaviour when generating compounds. In brief, here we limit the number to the top 25 candidates (this will save a lot of processing time), specify that we are primarily interested in protonated ions and use the pubchem database.

As a side note: this is often one of the most time consuming steps during the workflow. For this reason, and in order to not 'abuse' the server used by MetFrag, you should always take care to prioritize your data before running this function.

In the previous step we generated candidate formulas for all feature groups. These can be used to add yet another scoring parameter so we can assess if the formula part of a candidate structure makes sense: 

```{r,results='hide'}
compounds <- addFormulaScoring(compounds, formulas, updateScore = TRUE)
```

## Components

So far we mainly dealt with feature groups as if they are separate chemical compounds. However, formation of multiple ionization adducts (_e.g._ sodium and potassium) and the presence of isotopologues yield multiple _m/z_ values for the same compound. As a result, multiple feature groups will be  describing the same chemical compound. To reduce this complexity it is useful to generate so called _components_ which are basically groups of feature groups which are likely to be the same compound (based on _e.g._ similarity of retention profiles and known chemical rules). Another advantage of this grouping process is that extra chemical information of a compound is revealed that may help compound identification.

Generating components is performed with the `generateComponents()` function:

```{r components,results='hide'}
components <- generateComponents(fGroups, "camera", ionization = "positive")
```
```{r fig.width=5,fig.height=4}
components
plotSpec(components, "CMP6")
plotEIC(components, "CMP6", fGroups)
```

## Reporting

The last step of the workflow is typically reporting data: during this step all the collected data is transformed to graphical plots (`reportPDF()` and `reportMD()`) or tabular csv data (`reportCSV()`).

```{r eval=FALSE}
reportCSV(fGroups, formulas = formulas, compounds = compounds, components = components)
reportPDF(fGroups, formulas = formulas, compounds = compounds, components = components,
          MSPeakLists = plists)
reportMD(fGroups, formulas = formulas, compounds = compounds, components = components,
         MSPeakLists = plists)
```

The output of `reportMD()` can be viewed [here](../examples/report.html).

Note that these functions can be called at any time during the workflow. This may be especially useful if you want evaluate results during optimization or exploring the various algorithms available.

```{r echo=FALSE,eval=pkgdown::in_pkgdown()}
# do the actual reporting here

# ugly work around for nested rmarkdown call made by reportMD; based on https://gist.github.com/jennybc/1f747c5bb84aa9be9c3c
tempF <- tempfile(); tempScript <- tempfile(fileext = ".R")
save(fGroups, formulas, compounds, components, plists, file = tempF)
writeLines(sprintf('
library(patRoon)
setwd("%s")
load("%s")
options("patRoon.cache.fileName" = "%s")
#options(patRoon.path.pngquant = "~/werk/pngquant/")
reportMD(fGroups, path = "../docs/examples", formulas = formulas, compounds = compounds,
         components = components, MSPeakLists = plists, optimizePng = TRUE, openReport = FALSE)
', gsub("\\", "/", getwd(), fixed = TRUE), gsub("\\", "/", tempF, fixed = TRUE), getOption("patRoon.cache.fileName")), con = tempScript)
# devtools::clean_source(tempScript, quiet = TRUE)
callr::rscript(tempScript, show = FALSE)
```

## Final script

## Other topics

### Create new projects with `newProject()` { #newProject }

In this tutorial we started with creating a project directory with a template script and using the `generateAnalysisInfo()` to (semi-)automatically generate analysis information required for subsequent workflow steps. If you are using (a recent version of) R Studio a quicker and potentially user friendlier alternative is to use the `newProject()` function. This function will present you with a 'wizard' like graphical screen guiding you to choose a project directory, select all analyses and select all common workflow steps that should be executed. When finished the input will then be used to generate a template processing script.

### Suspect screening

```{r}
scr <- screenTargets(fGroups, patRoonData::targets)
head(scr)
```

```{r}
fGroupsScreening <- groupFeaturesScreening(fGroups, scr)
head(as.data.table(fGroupsScreening))
```

```{r fig.width=7,fig.height=5,results='hide'}
plotEIC(fGroupsScreening, colourBy = "fGroups", showFGroupRect = FALSE, showPeakArea = TRUE,
        topMost = 1, showLegend = TRUE)
```

```{r}
# perform 'regular' workflow steps as shown before (MS peak lists, formulas, compounds, ...)
```

### Import and exporting feature groups

### caching



```{r echo=FALSE,eval=!pkgdown::in_pkgdown()}
# cleanup
unlink("log", TRUE)
```



[patroonData]: https://github.com/rickhelmus/patRoonData
[readme]: https://rickhelmus.github.io/patRoon/
[ProteoWizard]: http://proteowizard.sourceforge.net/
[OpenMS]: http://openms.de/
[GenForm]: https://sourceforge.net/projects/genform/
[MetFrag]: http://ipb-halle.github.io/MetFrag/
[MetFragCL]: http://ipb-halle.github.io/MetFrag/projects/metfragcl/
